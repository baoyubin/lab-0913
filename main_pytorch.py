import pickleimport randomimport numpy as npimport torchimport torch.nn as nnimport transbigdata as tbdfrom matplotlib import pyplot as plt# import seaborn as snsfrom MapConfig import MapConfigfrom agent import Agentfrom my_env import Envfrom my_map import Bus_map, Load_mapfrom my_plot import My_plotimport mathimport timeimport seaborn as snsif __name__ == '__main__':    algorithm_mode = "DQN"    run_mode = 0 ##read the old network_params :  1    network_params = "./network_params/net_params_0.4_3000_3_22_28_15.pkl" ##修改delay_Weight    # with open('my_object.pkl', 'rb') as f:    #     loaded_object = pickle.load(f)    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    my_plot = My_plot()    config = MapConfig()    ##修改bounds/params 请先运行data_create    bounds = config.bounds    params = config.params    taxi_map = Load_map()    taxi_map.get_map(bounds, params)    bus_map = Bus_map()    bus_map.get_map(bounds, params)    map_x = math.ceil((bounds[2] - bounds[0]) / params['deltalon'])    map_y = math.ceil((bounds[3] - bounds[1]) / params['deltalat'])    if(run_mode == 1):        env = Env(map_x, map_y, taxi_map.load_map, bus_map.bus_map, my_plot, delay_Weight=0.2, idle_w=0.5, data_num=3)    else:        env = Env(map_x, map_y, taxi_map.load_map, bus_map.bus_map, my_plot)    s = env.reset()    assert (s is not env.observation_space)    EPSILON_DECAY = 100000    EPSILON_START = 1.0    EPSILON_END = 0.02    ##assx = len(taxi_map.load_map)    n_episode = 2000    n_time_step = 72  #(24-6)*6    ##assert (n_time_step == len(taxi_map.load_map)/10)    # n_state = len(s['taxi']) + len(s['bus']) * 2  # TODO    n_state = len(s['taxi']) + len(s['bus'])  # TODO    n_action = 2 ** env.action_num    aaa = []    agent = Agent(n_state, n_action, GAMA=0.9, learning_race=1e-4)    if(run_mode == 1):        agent.online_net.load_state_dict(torch.load(network_params))        agent.target_net.load_state_dict(torch.load(network_params))    REWARD_BUFFER = np.empty(n_episode)    TARGET_UPDATE_FREQUENCY = 5    start_time = time.time()    for episode_i in range(n_episode):        episode_reward = 0        # print(env.all_load)        # fig, ax = plt.subplots()        # # 使用seaborn绘制热点图        # sns.heatmap(env.all_load, ax=ax, annot=False, cmap='coolwarm')        # ax.invert_yaxis()        # # 显示图形        # plt.show()        # fig.savefig('heatmap.svg', format='svg')        for step_i in range(n_time_step):            if run_mode != 1:                epsilon = np.interp(episode_i * n_time_step + step_i, [0, EPSILON_DECAY],                                [EPSILON_START, EPSILON_END])            else:                epsilon = 0.02            if algorithm_mode == 'RANDOM':                epsilon = 1            random_sample = random.random()            if random_sample <= epsilon and episode_i != n_episode-1:                a = env.action_space_sample()                index = 0                for idx, val in enumerate(a):                    index += val * 2 ** (9 - idx)                oraction = index            else:                # merge = np.hstack((s['taxi'], s['bus'], s['bus_action']))                merge = np.hstack((s['taxi'], s['bus']))                a, oraction = agent.online_net.act(merge)  # TODO            ##ttt = taxi_map.load_map[env.T]            if episode_i == n_episode - 1:                aaa.append(sum(a))                print(random_sample, a)            s_, r, done, info = env.step(a)            assert (s is not s_)            # s_merge = np.hstack((s['taxi'], s['bus'], s['bus_action']))            # s__merge = np.hstack((s_['taxi'], s_['bus'], s_['bus_action']))            s_merge = np.hstack((s['taxi'], s['bus']))            s__merge = np.hstack((s_['taxi'], s_['bus']))            # agent.memo.add_memo(s_merge, a, r, done, s__merge)  # TODO            # batch_s_tensor = torch.as_tensor(np.asarray(batch_s), dtype=torch.float32)            # batch_a_tensor = torch.as_tensor(np.asarray(batch_a), dtype=torch.int64)            # batch_r_tensor = torch.as_tensor(np.asarray(batch_r), dtype=torch.float32).unsqueeze(-1)            # batch_done_tensor = torch.as_tensor(np.asarray(batch_done), dtype=torch.uint8).unsqueeze(-1)            # batch_s__tensor = torch.as_tensor(np.asarray(batch_s_), dtype=torch.float32)            ## PER DQN 特有的内容            policy_val = agent.online_net(torch.tensor(s_merge, device=device,dtype=torch.float32))[oraction]            target_val = agent.target_net(torch.tensor(s__merge, device=device,dtype=torch.float32))            if done:                error = abs(policy_val - r)            else:                error = abs(policy_val - r - 0.9 * torch.max(target_val))            agent.memo.push(error.cpu().detach().numpy(), (s_merge, a, r, s__merge, done))  # 保存transition            s = s_            episode_reward += r            if done:                s = env.reset()                REWARD_BUFFER[episode_i] = episode_reward                break            if len(agent.memo) < agent.memo.batch_size:  # 不满足一个批量时，不更新策略                continue                # 采样一个batch            (state_batch, action_batch, reward_batch, next_state_batch,             done_batch), idxs_batch, is_weights_batch = agent.memo.sample(                agent.memo.batch_size)            state_batch = torch.tensor(np.array(state_batch), device=device,                                       dtype=torch.float32)  # shape(batchsize,n_states)            action_batch = torch.tensor(np.array(action_batch), device=device,dtype=torch.int64)  # shape(batchsize,1)            reward_batch = torch.tensor(np.array(reward_batch), device=device, dtype=torch.float32).unsqueeze(                1)  # shape(batchsize,1)            next_state_batch = torch.tensor(np.array(next_state_batch), device=device,                                            dtype=torch.float32)  # shape(batchsize,n_states)            done_batch = torch.tensor(np.array(done_batch), device=device,dtype=torch.uint8).unsqueeze(1)            """映射二进制到相应的q_value"""  # TODO 转换规则一致            index = 0            ndarray = action_batch.numpy()            a_list = []            for i in ndarray:                index = 0                for idx, val in enumerate(i):                    index += val * 2 ** (9 - idx)                a_list.append(index)            a_list = torch.LongTensor(a_list).unsqueeze(-1)            # shape(batchsize,1)            q_value_batch = agent.online_net(state_batch).gather(dim=1,                                                                index=a_list)  # shape(batchsize,1),requires_grad=True            # next_max_q_value_batch = agent.target_net(next_state_batch).max(1)[0].detach().unsqueeze(1)            next_q_value_batch = agent.online_net(next_state_batch)  # 下一个状态对应的实际策略网络Q值            next_target_value_batch = agent.target_net(next_state_batch)  # 下一个状态对应的目标网络Q值            # 将策略网络Q值最大的动作对应的目标网络Q值作为期望的Q值            next_target_q_value_batch = next_target_value_batch.gather(1,                                                                       torch.max(next_q_value_batch, 1)[1].unsqueeze(1))            expected_q_value_batch = reward_batch + 0.9 * next_target_q_value_batch * (1 - done_batch)  # 期望的Q值            # expected_q_value_batch = reward_batch + 0.9 * next_max_q_value_batch * (1 - done_batch)            # loss中根据优先度进行了加权            loss = torch.mean(                torch.pow((q_value_batch - expected_q_value_batch) * torch.from_numpy(is_weights_batch).to(device),                          2))            # loss = nn.MSELoss()(q_value_batch, expected_q_value_batch)            abs_errors = np.sum(                np.abs(q_value_batch.cpu().detach().numpy() - expected_q_value_batch.cpu().detach().numpy()), axis=1)            # 需要更新样本的优先度            agent.memo.batch_update(idxs_batch, abs_errors)            # 反向传播            agent.optimizer.zero_grad()            loss.backward()            # 梯度截断，防止梯度爆炸            for param in agent.online_net.parameters():                param.grad.data.clamp_(-1, 1)            agent.optimizer.step()            # batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()            #            # # compute target            # if algorithm_mode == "DDQN":            #     online_q_value = agent.online_net(batch_s_)            #     index_max_online_q_value = online_q_value.max(dim=1, keepdim=True)[1]            #            #     target_q_value = agent.target_net(batch_s_)            #     max_target_q_value = torch.gather(input=target_q_value, dim=1, index=index_max_online_q_value)            #     y = batch_r + agent.GAMA * (1 - batch_done) * max_target_q_value            # else:            #     target_q_value = agent.target_net(batch_s_)            #     max_target_q_value = target_q_value.max(dim=1, keepdim=True)[0]            #     y = batch_r + agent.GAMA * (1 - batch_done) * max_target_q_value            #            # # loss            # q_value = agent.online_net(batch_s)            #            # """映射二进制到相应的q_value"""  # TODO 转换规则一致            # index = 0            # ndarray = batch_a.numpy()            # a_list = []            # for i in ndarray:            #     index = 0            #     for idx, val in enumerate(i):            #         index += val * 2 ** (9 - idx)            #     a_list.append(index)            # a_list = torch.LongTensor(a_list).unsqueeze(1)            # """================ """            # ##求a的q值            # a_q_value = torch.gather(input=q_value, dim=1, index=a_list)            # loss = nn.functional.smooth_l1_loss(y, a_q_value)            #            # # gradient            # agent.optimizer.zero_grad()            # loss.backward()            # agent.optimizer.step()            my_plot.loss_plot.append(loss.item())        if episode_i % TARGET_UPDATE_FREQUENCY == 0:            agent.target_net.load_state_dict(agent.online_net.state_dict())            print("Episode:{}".format(episode_i))            print("avg_reward_episode:{}".format(np.mean(REWARD_BUFFER[:episode_i + 1])))    total_time = time.time() - start_time    print('total_time:%s' % (total_time))    ##my_plot.plot_loss()    ##my_plot.plot_load()    ##my_plot.plot_avg_delay()    my_plot.params["delay_Weight"] = env.delay_Weight    my_plot.params["data_num"] = env.data_num    my_plot.params["idle_w"] = env.idle_w    my_plot.getData()    energy = [[item[0] for item in env.my_plot.cost_energy[-720:]],              [item[1] for item in env.my_plot.cost_energy[-720:]],              [item[2] for item in env.my_plot.cost_energy[-720:]]]    delay = [[item[0] for item in env.my_plot.cost_delay[-720:]],             [item[1] for item in env.my_plot.cost_delay[-720:]],             [item[2] for item in env.my_plot.cost_delay[-720:]]]    cost = [env.my_plot.system_cost[-72:],             env.my_plot.open_cost[-72:],             env.my_plot.close_cost[-72:]]    rr = [env.my_plot.reward_list[-72:],             env.my_plot.rw_all_1[-72:],             env.my_plot.rw_all_0[-72:]]    env,my_plot.plot_default(rr[0], [rr[1], rr[2],env.my_plot.rw_topk[-72:]], "reward")    env.my_plot.plot_default(cost[0], [cost[1], cost[2],env.my_plot.top_cost[-72:]], "cost")    env.my_plot.plot_default(delay[0], [delay[1], delay[2],env.my_plot.top_delay[-720:]], "delay")    ##[cost_delay, all_1_delay, all_0_delay]    env.my_plot.plot_default(energy[0], [energy[1], energy[2],env.my_plot.top_energy[-720:]], "energy")    c = [(delay[2][i] - delay[1][i]) / delay[2][i] for i in range(0, len(delay[0]))]    d = [(energy[1][i] - energy[2][i]) / energy[1][i] for i in range(0, len(energy[0]))]    my_plot.param = [config.taxi_path,agent.memo.BATCH_SIZE,env.delay_Weight,env.rw_method,env.maxenergy,TARGET_UPDATE_FREQUENCY,agent.memo.MEMORY_SIZE,agent.learning_race]    print("END")    print(episode_reward)    print(env.delay_Weight, env.data_num,  env.idle_w)    print("==============")    print("dealy:", sum(c)/len(c), "energy", sum(d)/len(d))    print(aaa)    lllll = str(time.strftime('%H_%M_%S', time.localtime(time.time())))    env.my_plot.plot_load()    if n_episode != 1:        my_plot.plot_reward()        my_plot.plot_cost()        if algorithm_mode == "DDQN":            torch.save(agent.online_net.state_dict(), 'ddqn/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)                       + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('ddqn/' + '_' + str(env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd / env.B_bus)                       + '_' + str(env.ECD_CPU_frequency / env.BUS_CPU_frequency) + '_' + str(                env.bus_bound) + lllll + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy,                        delay], fmt='%s')        elif algorithm_mode == 'RANDOM':            torch.save(agent.online_net.state_dict(),                       'random/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)                       + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('random/' + lllll + '_' + str(env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd / env.B_bus)                       + '_' + str(env.ECD_CPU_frequency / env.BUS_CPU_frequency) + '_' + str(env.bus_bound) + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy, delay], fmt='%s')        else:            # torch.save(agent.online_net.state_dict(),            #            'network_params/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)            #            + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('network_params/' + str(agent.memo.BATCH_SIZE) + '_' + str(agent.learning_race) + '_' + str(                env.busidle) + '_' + str(n_time_step) + ' _ ' + str(max(max(delay))) + '_' + str(                env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd)                       + '_' + str(env.ECD_CPU_frequency) + '_' + str(env.bus_bound) + '-' + str(                n_time_step) + lllll + '_' + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy,                        delay], fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_' + lllll + '_' + '.txt',                [aaa], delimiter=',', fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_delay' + lllll + '_' + '.txt',                [env.my_plot.maxmindelay[-720:]], delimiter=',', fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_energy' + lllll + '_' + '.txt',                [env.my_plot.maxminenergy[-720:]], delimiter=',', fmt='%s')