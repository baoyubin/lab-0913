import pickleimport randomimport numpy as npimport pandas as pdimport torchimport torch.nn as nnimport transbigdata as tbdfrom matplotlib import pyplot as plt# import seaborn as snsfrom MapConfig import MapConfigfrom agent import Agentfrom my_env import Envfrom my_map import Bus_map, Load_mapfrom my_plot import My_plotimport mathimport timeimport seaborn as snsif __name__ == '__main__':    seed = 114514    np.random.seed(seed)    random.seed(seed)    torch.manual_seed(seed)  # config for CPU    torch.cuda.manual_seed(seed)  # config for GPU    torch.backends.cudnn.deterministic = True    torch.backends.cudnn.benchmark = False    algorithm_mode = "DDQN"    run_mode = 0 ##read the old network_params :  1    network_params = "./network_params/net_params_0.4_3000_3_22_28_15.pkl" ##修改delay_Weight    # with open('my_object.pkl', 'rb') as f:    #     loaded_object = pickle.load(f)    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    my_plot = My_plot()    config = MapConfig()    ##修改bounds/params 请先运行data_create    bounds = config.bounds    params = config.params    taxi_map = Load_map()    taxi_map.get_map(bounds, params)    bus_map = Bus_map()    bus_map.get_map(bounds, params)    map_x = math.ceil((bounds[2] - bounds[0]) / params['deltalon'])    map_y = math.ceil((bounds[3] - bounds[1]) / params['deltalat'])    if(run_mode == 1):        env = Env(map_x, map_y, taxi_map.load_map, bus_map.bus_map, my_plot, delay_Weight=0.2, idle_w=0.5, data_num=3)    else:        env = Env(map_x, map_y, taxi_map.load_map, bus_map.bus_map, my_plot)    s = env.reset()    assert (s is not env.observation_space)    EPSILON_DECAY = 100000    EPSILON_START = 1.0    EPSILON_END = 0.02    ##assx = len(taxi_map.load_map)    n_episode = 2000    n_time_step = 72  #(24-6)*6    ##assert (n_time_step == len(taxi_map.load_map)/10)    # n_state = len(s['taxi']) + len(s['bus']) * 2  # TODO    n_state = len(s['taxi']) + len(s['bus'])  # TODO    n_action = 2 ** env.action_num    aaa = []    agent = Agent(n_state, n_action, GAMA=0.9, learning_race=1e-4)    if(run_mode == 1):        agent.online_net.load_state_dict(torch.load(network_params))        agent.target_net.load_state_dict(torch.load(network_params))    REWARD_BUFFER = np.empty(n_episode)    TARGET_UPDATE_FREQUENCY = 5    start_time = time.time()    for episode_i in range(n_episode):        episode_reward = 0        # print(env.all_load)        # fig, ax = plt.subplots()        # # 使用seaborn绘制热点图        # sns.heatmap(env.all_load, ax=ax, annot=False, cmap='coolwarm')        # ax.invert_yaxis()        # # 显示图形        # plt.show()        # fig.savefig('heatmap.svg', format='svg')        for step_i in range(n_time_step):            if run_mode != 1:                epsilon = np.interp(episode_i * n_time_step + step_i, [0, EPSILON_DECAY],                                [EPSILON_START, EPSILON_END])            else:                epsilon = 0.02            if algorithm_mode == 'RANDOM':                epsilon = 1            random_sample = random.random()            if random_sample <= epsilon and episode_i != n_episode-1:                a = env.action_space_sample()            else:                # merge = np.hstack((s['taxi'], s['bus'], s['bus_action']))                merge = np.hstack((s['taxi'], s['bus'],))                a = agent.online_net.act(merge)  # TODO            ##ttt = taxi_map.load_map[env.T]            if episode_i == n_episode - 1:                aaa.append(sum(a))                print(random_sample, a)            s_, r, done, info = env.step(a)            assert (s is not s_)            # s_merge = np.hstack((s['taxi'], s['bus'], s['bus_action']))            # s__merge = np.hstack((s_['taxi'], s_['bus'], s_['bus_action']))            s_merge = np.hstack((s['taxi'], s['bus']))            s__merge = np.hstack((s_['taxi'], s_['bus']))            agent.memo.add_memo(s_merge, a, r, done, s__merge)  # TODO            s = s_            episode_reward += r            batch_s, batch_a, batch_r, batch_done, batch_s_ = agent.memo.sample()            # compute target            if algorithm_mode == "DDQN":                online_q_value = agent.online_net(batch_s_)                index_max_online_q_value = online_q_value.max(dim=1, keepdim=True)[1]                target_q_value = agent.target_net(batch_s_)                max_target_q_value = torch.gather(input=target_q_value, dim=1, index=index_max_online_q_value)                y = batch_r + agent.GAMA * (1 - batch_done) * max_target_q_value            else:                target_q_value = agent.target_net(batch_s_)                max_target_q_value = target_q_value.max(dim=1, keepdim=True)[0]                y = batch_r + agent.GAMA * (1 - batch_done) * max_target_q_value            # loss            q_value = agent.online_net(batch_s)            """映射二进制到相应的q_value"""  # TODO 转换规则一致            index = 0            ndarray = batch_a.numpy()            a_list = []            for i in ndarray:                index = 0                for idx, val in enumerate(i):                    index += val * 2 ** (9 - idx)                a_list.append(index)            a_list = torch.LongTensor(a_list).unsqueeze(1)            """================ """            ##求a的q值            a_q_value = torch.gather(input=q_value, dim=1, index=a_list)            loss = nn.functional.smooth_l1_loss(y, a_q_value)            # gradient            agent.optimizer.zero_grad()            loss.backward()            agent.optimizer.step()            my_plot.loss_plot.append(loss.item())            if done:                s = env.reset()                REWARD_BUFFER[episode_i] = episode_reward                break        if episode_i % TARGET_UPDATE_FREQUENCY == 0:            agent.target_net.load_state_dict(agent.online_net.state_dict())            print("Episode:{}".format(episode_i))            print("avg_reward_episode:{}".format(np.mean(REWARD_BUFFER[:episode_i + 1])))    total_time = time.time() - start_time    print('total_time:%s' % (total_time))    ##my_plot.plot_loss()    ##my_plot.plot_load()    ##my_plot.plot_avg_delay()    my_plot.params["delay_Weight"] = env.delay_Weight    my_plot.params["data_num"] = env.data_num    my_plot.params["idle_w"] = env.idle_w    my_plot.getData()    energy = [[item[0] for item in env.my_plot.cost_energy[-720:]],              [item[1] for item in env.my_plot.cost_energy[-720:]],              [item[2] for item in env.my_plot.cost_energy[-720:]]]    delay = [[item[0] for item in env.my_plot.cost_delay[-720:]],             [item[1] for item in env.my_plot.cost_delay[-720:]],             [item[2] for item in env.my_plot.cost_delay[-720:]]]    cost = [env.my_plot.system_cost[-72:],             env.my_plot.open_cost[-72:],             env.my_plot.close_cost[-72:]]    rr = [env.my_plot.reward_list[-72:],             env.my_plot.rw_all_1[-72:],             env.my_plot.rw_all_0[-72:]]    env,my_plot.plot_default(rr[0], [rr[1], rr[2],env.my_plot.rw_topk[-72:]], "reward")    env.my_plot.plot_default(cost[0], [cost[1], cost[2],env.my_plot.top_cost[-72:]], "cost")    env.my_plot.plot_default(delay[0], [delay[1], delay[2],env.my_plot.top_delay[-720:]], "delay")    ##[cost_delay, all_1_delay, all_0_delay]    env.my_plot.plot_default(energy[0], [energy[1], energy[2],env.my_plot.top_energy[-720:]], "energy")    c = [(delay[2][i] - delay[1][i]) / delay[2][i] for i in range(0, len(delay[0]))]    d = [(energy[1][i] - energy[2][i]) / energy[1][i] for i in range(0, len(energy[0]))]    my_plot.param = [config.taxi_path,agent.memo.BATCH_SIZE,env.delay_Weight,env.rw_method,env.maxenergy,TARGET_UPDATE_FREQUENCY,agent.memo.MEMORY_SIZE,agent.learning_race]    print("END")    print(episode_reward)    print(env.delay_Weight, env.data_num,  env.idle_w)    print("==============")    print("dealy:", sum(c)/len(c), "energy", sum(d)/len(d))    print(aaa)    lllll = str(time.strftime('%H_%M_%S', time.localtime(time.time())))    env.my_plot.plot_load()    if n_episode != 1:        my_plot.plot_reward()        my_plot.plot_cost()        if algorithm_mode == "DDQN":            torch.save(agent.online_net.state_dict(), 'ddqn/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)                       + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('ddqn/' + '_' + str(env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd / env.B_bus)                       + '_' + str(env.ECD_CPU_frequency / env.BUS_CPU_frequency) + '_' + str(                env.bus_bound) + lllll + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy,                        delay], fmt='%s')        elif algorithm_mode == 'RANDOM':            torch.save(agent.online_net.state_dict(),                       'random/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)                       + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('random/' + lllll + '_' + str(env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd / env.B_bus)                       + '_' + str(env.ECD_CPU_frequency / env.BUS_CPU_frequency) + '_' + str(env.bus_bound) + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy, delay], fmt='%s')        else:            # torch.save(agent.online_net.state_dict(),            #            'network_params/net_params_' + str(env.delay_Weight) + '_' + str(n_episode)            #            + '_' + str(env.data_num) + '_' + lllll + '.pkl')  # 只保存网络中的参数            np.savetxt('network_params/' + str(agent.memo.BATCH_SIZE) + '_' + str(agent.learning_race) + '_' + str(                env.busidle) + '_' + str(n_time_step) + ' _ ' + str(max(max(delay))) + '_' + str(                env.delay_Weight) + '_' + str(n_episode) + '_' + str(                env.data_num) + '_' + str(env.B_ecd)                       + '_' + str(env.ECD_CPU_frequency) + '_' + str(env.bus_bound) + '-' + str(                n_time_step) + lllll + '_' + '.txt',                       [env.my_plot.reward_list[-n_time_step:], env.my_plot.rw_all_1[-n_time_step:],                        env.my_plot.rw_all_0[-n_time_step:], energy,                        delay], fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_' + lllll + '_' + '.txt',                [aaa], delimiter=',', fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_delay' + lllll + '_' + '.txt',                [env.my_plot.maxmindelay[-720:]], delimiter=',', fmt='%s')            np.savetxt(                'network_params/' + str(max(max(delay))) + '_energy' + lllll + '_' + '.txt',                [env.my_plot.maxminenergy[-720:]], delimiter=',', fmt='%s')